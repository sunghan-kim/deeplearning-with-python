{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 신경망의 수학적 구성 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 신경망의 엔진: 그래디언트 기반 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**각 층의 입력 데이터 변환**\n",
    "\n",
    "```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "\n",
    "\n",
    "- 이 식에서 텐서 `W`와 `b`\n",
    "  - 층의 속성\n",
    "  - `W` : 가중치(weight) 또는 훈련되는 파라미터(trainable parameter)\n",
    "  - `b` : 편향 (bias)\n",
    "  - 이런 가중치에는 훈련 데이터를 신경망에 노출시켜서 학습된 정보가 담겨 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**무작위 초기화(random initialization)**\n",
    "\n",
    "- 초기에는 가중치 행렬이 작은 난수로 채워져 있음\n",
    "- 피드백 신호에 기초하여 가중치가 점진적으로 조정될 것\n",
    "- 이런 점진적인 조정 또는 **훈련(training)**이 머신 러닝 학습의 핵심이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id=\"tl\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**훈련 반복 루프 (training loop)**\n",
    "\n",
    "- 훈련은 다음과 같은 훈련 반복 루프 안에서 일어난다.\n",
    "- 필요한 만큼 반복 루프 안에서 이런 단계가 반복됨  \n",
    "  \n",
    "  \n",
    "1. 훈련 샘플 x와 이에 상응하는 타깃 y의 배치를 추출\n",
    "2. x를 사용하여 네트워크를 실행하고(정방향 패스(forward pass) 단계), 예측 y_pred를 구함\n",
    "3. y_pred와 y의 차이를 측정하여 이 배치에 대한 네트워크의 손실을 계산\n",
    "4. 배치에 대한 손실이 조금 감소되도록 네트워크의 모든 가중치를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4단계: 가중치의 업데이트**\n",
    "\n",
    "- 개별적인 가중치 값이 있을 때 값이 증가해야 할지 감소해야 할지, 또 얼만큼 업데이트해야 할 지 알 수 있을까?  \n",
    "  \n",
    "  \n",
    "1. 네트워크 가중치 행렬의 원소를 모두 고정하고 관심 있는 하나만 다른 값을 적용\n",
    "  - 이 가중치의 초깃값이 0.3이라고 가정\n",
    "  - 배치 데이터를 정방향 패스에 통과시킴 $\\Rightarrow$ 네트워크의 손실이 0.5가 나옴\n",
    "  - 이 가중치 값을 0.35로 변경하고 다시 정방향 패스를 실행 $\\Rightarrow$ 네트워크 손실이 0.6으로 증가  \n",
    "  (가중치 증가 $\\Rightarrow$ 손실 증가)\n",
    "  - 가중치 값을 0.25로 줄이고 정방향 패스 실행 $\\Rightarrow$ 네트워크 손실이 0.4로 감소  \n",
    "  (가중치 감소 $\\Rightarrow$ 손실 감소)\n",
    "  - 이 경우에 가중치를 -0.05 만큼 업데이트한 것이 손실을 줄이는 데 기여한 것으로 보임  \n",
    "  - 이런 식으로 네트워크의 모든 가중치에 반복  \n",
    "  - 이 방식은 모든 가중치 행렬의 원소마다 두 번의 정방향 패스를 계산해야 하므로 엄청나게 비효율적이다.  \n",
    "  \n",
    "  \n",
    "2. **그래디언트(gradient)** 계산\n",
    "  - 신경망에 사용된 모든 연산이 **미분 가능(differentiable)**하다는 장점을 사용하여 네트워크의 가중치에 대한 손실의 **그래디언트(gradient, 연산의 미분 결과)**를 계산하는 것이 훨씬 더 좋은 방법\n",
    "  - 그래디언트의 반대 방향으로 가중치를 이동하면 손실이 감소된다.  \n",
    "  (그래디언트 증가 $\\Rightarrow$ 가중치 감소 $\\Rightarrow$ 손실 감소)  \n",
    "  (그래디언트 감소 $\\Rightarrow$ 가중치 증가 $\\Rightarrow$ 손실 감소) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 변화율이란?\n",
    "\n",
    "**연속성의 개념**\n",
    "\n",
    "- $f(x) = y$\n",
    "  - 실수 $x$를 새로운 실수 $y$로 매핑하는 연속적이고 매끄러운 함수\n",
    "  - 이 함수가 연속적이므로 $x$를 조금 바꾸면 $y$가 조금만 변경될 것이다.\n",
    "  - 이 것이 연속성의 개념이다.  \n",
    "  \n",
    "  \n",
    "- $x$를 작은 값 $epsilon\\_x$ 만큼 증가시켰을 때 $y$가 $epsilon\\_y$ 만큼 바뀐다고 할 수 있다.\n",
    "\n",
    "$$\n",
    "f\\left( x + epsilon\\_x \\right) = y + a \\times epsilon\\_x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**변화율(derevative)**\n",
    "\n",
    "- 이 선형적인 근사는 $x$가 $p$에 충분히 가까울 때 유효하다.\n",
    "- 이 기울기를 $p$에서 $f$의 **변화율**이라고 한다.  \n",
    "  \n",
    "  \n",
    "- 이는 $a$가 음수일 때 $p$에서 양수$x$ 만큼 조금 이동하면 $f(x)$가 감소한다는 것을 의미한다.\n",
    "\n",
    "<img src=\"img/img_2-10.jpg\" width=\"300px\" />\n",
    "\n",
    "- $a$가 양수일 때는 음수 $x$ 만큼 조금 이동하면 $f(x)$ 가 감소된다.  \n",
    "  \n",
    "  \n",
    "- $a$의 절댓값(변화율의 크기)은 이런 증가나 감소가 얼마나 빠르게 일어날 지 알려준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**미분 가능**\n",
    "\n",
    "- 미분 가능하다  \n",
    "  - 변화율을 유도할 수 있다\n",
    "  - 예를 들어 매끄럽고 연속적인 함수 일 때 이 함수는 미분 가능하다고 한다.\n",
    "  \n",
    "  \n",
    "- 모든 미분 가능한 함수 $f(x)$ 에 대해 $x$의 값을 $f$의 국부적인 선형 근사인 그 지점의 기울기로 매핑하는 변화율 함수 $f'(x)$가 존재한다.\n",
    "  - $cos(x)$의 변화율 $\\Rightarrow$ $-sin(x)$\n",
    "  - $f(x) = a \\times x$의 변화율 $\\Rightarrow$ $f'(x) = a$  \n",
    "  \n",
    "  \n",
    "- $f(x)$ 를 최소화하기 위해 $epsilon\\_x$ 만큼 $x$를 업데이트하고 싶을 때 $f$의 변화율을 알고 있다면 해결된다.\n",
    "- 변화율 함수는 $x$가 바뀜에 따라 $f(x)$가 어떻게 바뀔지 설명해 준다.\n",
    "- $f(x)$의 값을 감소시키고 싶다면 $x$를 변화율의 방향과 반대로 조금 이동해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 텐서 연산의 변화율: 그래디언트\n",
    "\n",
    "**그래디언트**\n",
    "\n",
    "- 텐서 연산의 변화율\n",
    "- 다차원 입력, 즉 텐서를 입력으로 받는 힘수에 변화율 개념을 확장시킨 것    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**손실 함수**\n",
    "\n",
    "- 입력 벡터 `x`, 행렬 `W`, 타깃 `y`와 손실 함수 `loss`가 있다고 가정\n",
    "- `W`를 사용하여 타깃의 예측 `y_pred`를 계산\n",
    "- 손실, 즉 타깃 예측 `y_pred`와 타깃 `y` 사이의 오차를 계산할 수 있음\n",
    "\n",
    "```python\n",
    "y_pred = dot(W, x)\n",
    "loss_value = loss(y_pred, y)\n",
    "```\n",
    "\n",
    "- 입력 데이터 `x`와 `y`가 고정되어 있다면 이 함수는 `W`를 손실 값에 매핑하는 함수로 볼 수 있다.\n",
    "\n",
    "```python\n",
    "loss_value = f(W)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그래디언트와 손실 함수의 관계**\n",
    "\n",
    "- `W0` : `W`의 현재값\n",
    "- 포인트 `W0`에서 `f`의 변화율 = `W`와 같은 크기의 텐서인 `gradient(f)(W0)`\n",
    "(`gradient(f)(W0)` : `f`의 변화율 함수를 `gradient(f)`라고 했을 때 `W0` 지점의 그래디언트 텐서를 의미)\n",
    "- 이 텐서의 각 원소 `gradient(f)(W0)[i, j]`는 `W0[i, j]`를 변경했을 때 `loss_value`가 바뀌는 방향과 크기를 나타낸다.\n",
    "- 다시 말해 텐서 `gradient(f)(W0)`가 `W0`에서 함수 `f(W) = loss_value`의 그래디언트이다.  \n",
    "  \n",
    "  \n",
    "- 함수 $f(x)$의 변화율 하나는 곡선 $f$의 기울기로 해석할 수 있다.\n",
    "- 비슷하게 `gradient(f)(W0)`는 `W0`에서 `f(W)`의 기울기를 나타내는 텐서로 해석할 수 있다.  \n",
    "  \n",
    "  \n",
    "  \n",
    "- 함수 $f(x)$에 대해서는 변화율의 반대 방향으로 $x$를 조금 움직이면 $f(x)$의 값을 감소시킬 수 있다.\n",
    "- 동일한 방식을 적용하면 함수 `f(W)`의 입장에서는 그래디언트의 반대 방향으로 W를 움직이면 `f(W)`의 값을 줄일 수 있다.\n",
    "- 여기에서는 입력 x에 대한 미분을 **변화율(derivative)**이라고 하며, x를 상수로 생각하고 `W`에 대해 미분한 것을 **그래디언트(gradient)**로 표현하고 있다.  \n",
    "  \n",
    "  \n",
    "- 예를 들어 `W1`은 다음과 같이 구할 수 있다.\n",
    "\n",
    "```python\n",
    "W1 = W0 - step * gradient(f)(W0)\n",
    "```\n",
    "\n",
    "- `step` : 스케일을 조정하기 위한 작은 값  \n",
    "  \n",
    "  \n",
    "- 기울기가 작아지는 곡면의 낮은 위치로 이동된다.  \n",
    "- `gradient(f)(W0)`는 `W0`에 아주 가까이 있을 때 기울기를 근사한 것이므로 `W0`에서 너무 크게 벗어나지 않기 위해 스케일링 비율 `step`이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 확률적 경사 하강법\n",
    "\n",
    "**미분 가능한 함수의 최솟값**\n",
    "\n",
    "- 미분 가능한 함수가 주어지면  이론적으로 이 함수의 최솟값을 해석적으로 구할 수 있다.\n",
    "- 함수의 최솟값 == 변화율이 0인 지점\n",
    "- 변화율이 0이 되는 지점을 모두 찾고, 이 중에서 어떤 포인트의 함수 값이 가장 작은 지 확인\n",
    "- 신경망에 적용하면 가장 작은 손실 함수의 값을 만드는 가중치의 조합을 해석적으로 찾는 것을 의미\n",
    "- 이는 아래 식을 풀면 해결된다.\n",
    "\n",
    "```python\n",
    "gradient(f)(W) = 0\n",
    "```\n",
    "\n",
    "- 이 식은 $N$개의 변수로 이루어진 다항식이다. ($N$ : 네트워크의 가중치 개수)\n",
    "- 실제 신경망에는 파라미터의 개수가 종종 수천만 개가 되기 때문에 해석적으로 해결하는 것이 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[훈련 반복 루프](#tl)를 사용한 가중치 업데이트**\n",
    "\n",
    "- 랜덤한 배치 데이터에서 현재 손실 값을 토대로 하여 조금씩 파라미터를 수정\n",
    "- 미분 가능한 함수를 가지고 있으므로 그래디언트를 계산하여 [훈련 반복 루프](#tl)의 단계 4를 효율적으로 구현할 수 있다.ㅏ\n",
    "- 그래디언트의 반대 방향으로 가중치를 업데이트하면 손실이 매번 조금씩 감소할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**미니 배치 확률적 경사 하강법 (mini-batch stochastic gradient descent, 미니 배치 SGD)**\n",
    "\n",
    "1. 훈련 샘플 패치 x와 이에 상응하는 타깃 y를 추출\n",
    "2. x로 네트워크를 실행하고 예측 y_pred를 구함\n",
    "3. 이 배치에서 y_pred와 y 사이의 오차를 측정하여 네트워크의 손실을 계산\n",
    "4. 네트워크의 파라미터에 대한 손실 함수의 그래디언트를 계산 (**역방향 패스(backward pass)**)\n",
    "5. 그래디언트의 반대 방향으로 파라미터를 조금 이동시킴\n",
    "  - 예를 들어 `W -= step * gradient`처럼 하면 배치에 대한 손실이 조금 감소할 것\n",
    "  \n",
    "- **확률적(stochastic)**\n",
    "  - 각 배치 데이터가 무작위로 선택된다는 의미\n",
    "  - 확률적이란 것은 무작위(random)하다는 것의 과학적 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**네트워크의 파라미터와 훈련 샘플이 하나일 때의 과정**\n",
    "\n",
    "- SGD가 1D 손실 함수(1개의 학습 파라미터)의 값을 낮춘다.\n",
    "\n",
    "<img src=\"img/img_2-11.jpg\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**step 의 크기 선택**\n",
    "\n",
    "- step 값을 적절히 고르는 것이 중요하다.\n",
    "\n",
    "1. step의 크기가 너무 작은 경우\n",
    "  - 곡선을 따라 내려가는 데 너무 많은 반복이 필요\n",
    "  - 지역 최솟값 (local minimum)에 갇힐 수 있다.  \n",
    "  \n",
    "  \n",
    "2. step의 크기가 너무 큰 경우\n",
    "  - 손실 함수 곡선에서 완전히 임의의 위치로 이동시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**진정한(true) SGD**\n",
    "\n",
    "- 미니 배치 SGD 알고리즘의 변종\n",
    "- 반복마다 하나의 샘플과 하나의 타깃을 뽑는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**배치 SGD (batch SGD)**\n",
    "\n",
    "- 가용한 모든 데이터를 사용하여 반복을 실행할 수 있다.\n",
    "- 더 정확하게 업데이터되지만 더 많은 비용이 든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**극단적인 두 가지 방법(진정한 SGD vs 배치 SGD)의 효율적인 절충안**\n",
    "\n",
    "- 적절한 크기의 미니 배치를 사용하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**여러 가지 종류의 옵티마이저**\n",
    "\n",
    "- 업데이트한 다음 가중치를 계산할 때 현재 그래디언트 값만 보지 않고 이전에 업데이트된 가중치를 여러 가지 다른 방식으로 고려하는 SGD 변종이 많이 있다.\n",
    "  - 모멘텀을 사용한 SGD\n",
    "  - Adagrad\n",
    "  - RMSProp  \n",
    "  \n",
    "  \n",
    "- 이런 변종들을 모두 **최적화 방법(optimization method)** 또는 **옵티마이저**라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모멘텀(momentum)**\n",
    "\n",
    "- 특히 여러 변종들에서 사용하는 모멘텀의 개념은 아주 중요하다.\n",
    "- 모멘텀은 SGD에 있는 2개의 문제점인 **수렴 속도**와 **지역 최솟값**을 해결한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**네트워크의 파라미터 하나에 대한 손실 값 곡선**\n",
    "\n",
    "<img src=\"img/img_2-13.jpg\" width=\"300px\" />\n",
    "\n",
    "- 어떤 파라미터 값에서는 지역 최솟값에 도달한다.\n",
    "- 그 지점 근처에서는 왼쪽으로 이동해도 손실이 증가하고, 오른쪽으로 이동해도 손실이 증가한다.\n",
    "- 대상 파라미터가 작은 학습률을 가진 SGD로 최적화되었다면 최적화 과정이 전역 최솟값으로 향하지 못하고 이 지역 최솟값에 갇히게 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 변화율 연결: 역전파 알고리즘\n",
    "\n",
    "- 앞의 알고리즘에서 함수가 미분 가능하기 때문에 변화율을 직접 계산할 수 있다고 가정함\n",
    "- 실제로 신경망은 많은 텐서 연산으로 구성되어 있고 이 연산들의 변화율은 간단하며 이미 잘 알려져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**역전파 (Backpropagation) 알고리즘**\n",
    "\n",
    "- 3개의 텐서 연산 `a`, `b`, `c`와 가중치 행렬 `W1`, `W2`, `W3`로 구성된 네트워크 `f`\n",
    "\n",
    "```python\n",
    "f(W1, W2, W3) = a(W1, b(W2, c(W3)))\n",
    "```\n",
    "\n",
    "- 미적분에서 이렇게 연결된 함수는 **연쇄 법칙(chain rule)**이라고 부르는 다음 항등식을 사용해 유도될 수 있다.\n",
    "\n",
    "$$\n",
    "f(g(x))' = f'(g(x)) \\cdot g'(x)\n",
    "$$\n",
    "\n",
    "- 연쇄 법칙을 신경망의 그래디언트 계산에 적용하여 **역전파 알고리즘(후진 모드 자동 미분, reverse-mode autoomatic differentiation)**이 탄생하게 되었다.\n",
    "\n",
    "- 역전파는 최종 손실 값에서부터 시작한다.\n",
    "- 손실 값에 각 파라미터가 기여한 정도를 계산하기 위해 연쇄 법칙을 적용하여 최상위 층에서 하위층까지 거꾸로 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기호 미분 (symbolic differentiation)**\n",
    "\n",
    "- 요즘에는 그리고 향후 몇 년 동안은 텐서플로처럼 **기호 미분**이 가능한 최신 프레임워크를 사용하여 신경망을 구현할 것이다.\n",
    "- 이 말은 변화율이 알려진 연산들로 연결되어 있으면 (연쇄 법칙을 적용하여) 네트워크 파라미터와 그래디언트 값을 매핑하는 그래디언트 함수를 계산할 수 있다는 의미이다.\n",
    "- 이런 함수를 사용하면 역방향 패스는 그래디언트 함수를 호출하는 것으로 단순화될 수 있다.\n",
    "- 기호 미분 덕택에 역전파 알고리즘을 직접 구현할 필요가 없고 그래디언트 기반의 최적화가 어떻게 작동하는 지 잘 이해하는 것으로 충분하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
